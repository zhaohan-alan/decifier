import os
import gc
import pickle
import argparse
from pymatgen.io.cif import CifWriter, Structure, CifParser
from pymatgen.core import Composition
from typing import List, Optional
from dscribe.descriptors import SOAP, ACSF

try:
    parser_from_string = CifParser.from_str
except:
    parser_from_string = CifParser.from_string

from pymatgen.analysis.diffraction.xrd import XRDCalculator
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from glob import glob
import multiprocessing as mp
from multiprocessing import Pool, cpu_count, TimeoutError
import numpy as np
import torch

import h5py
import gzip
import json

import logging
from logging import handlers

from decifer.tokenizer import Tokenizer
from decifer.cl_model import CLEncoder
from decifer.utility import (
    disc_to_cont_xrd,
    replace_symmetry_loop_with_P1,
    remove_cif_header,
    remove_oxidation_loop,
    format_occupancies,
    extract_formula_units,
    extract_formula_nonreduced,
    extract_space_group_symbol,
    replace_data_formula_with_nonreduced_formula,
    round_numbers,
    add_atomic_props_block,
)

import warnings
warnings.filterwarnings("ignore")

def init_worker(log_queue=None, model_path=None, device='cpu'):
    """
    Initializes each worker process with a queue-based logger and loads the model if provided.
    This will be called when each worker starts.
    """
    global xrd_encoder

    # Initialize logger
    if log_queue:
        queue_handler = handlers.QueueHandler(log_queue)
        logger = logging.getLogger()
        logger.addHandler(queue_handler)
        logger.setLevel(logging.ERROR)
    else:
        logger = logging.getLogger()
        logger.setLevel(logging.ERROR)

    if model_path:
        # Load the model
        checkpoint = torch.load(model_path, map_location=device)
        model_args = checkpoint['model_args']

        # Initialize the encoder model with the model args from checkpoint
        xrd_encoder = CLEncoder(
            embedding_dim=model_args["embedding_dim"],
            proj_dim=model_args["proj_dim"],
            qmin=model_args["qmin"],
            qmax=model_args["qmax"],
            qstep=model_args["qstep"],
            fwhm_range=model_args["fwhm_range"],
            noise_range=model_args["noise_range"],
            intensity_scale_range=model_args["intensity_scale_range"],
            mask_prob=model_args["mask_prob"]
        )

        # Load model state dict
        xrd_encoder.load_state_dict(checkpoint["model"])
        xrd_encoder.to(device)
        xrd_encoder.eval()

def log_listener(queue, log_dir):
    """
    Function excecuted by the log listener process.
    Receives messages from the queue and writes them to the log file.
    Rotates the log file based on size.
    """
    # Setup file handler (rotating)
    log_file = os.path.join("./" + log_dir, "error.log")
    handler = handlers.RotatingFileHandler(
        log_file, maxBytes=1024*1024, backupCount=5,
    )
    handler.setLevel(logging.ERROR)

    # Add a formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)

    # Set up the log listener
    listener = handlers.QueueListener(queue, handler)
    listener.start()

    # return listener
    return listener

def save_metadata(metadata, data_dir):
    """
    Save metadata information (sizes, shapes, argument parameters, vocab size, etc.) into a centralized JSON file.

    Args:
        metadata (dict): Dictionary containing metadata information.
        data_dir (str): Directory where the metadata file should be saved.
    """
    metadata_file = os.path.join(data_dir, "metadata.json")
    
    # If the metadata file already exists, load the existing metadata and update it
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r') as f:
            existing_metadata = json.load(f)
        # Update existing metadata with new data
        existing_metadata.update(metadata)
        metadata = existing_metadata
    
    # Save updated metadata to the JSON file
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=4)
    
def create_stratification_key(pmg_structure, group_size):
    """
    Create a stratification key from the spacegroup.

    Args:
        pmg_structure: pymatgen Structure object.
        group_size (int): Size of the spacegroup bin.

    Returns:
        str: Stratification key combining spacegroup in defined groups.
    """
    
    spacegroup_number = pmg_structure.get_space_group_info()[1]
    group_start = ((spacegroup_number - 1) // group_size) * group_size + 1
    group_end = group_start + group_size - 1

    return f"{group_start}-{group_end}"

def safe_pkl_gz(output, output_path):
    temp_path = output_path + '.tmp' # Ensuring that only fully written files are considered when collecting
    with gzip.open(temp_path, 'wb') as f:
        pickle.dump(output, f)
    os.rename(temp_path, output_path) # File is secure, renaming

def run_subtasks(
    root: str,
    worker_function,
    get_from: str,
    save_to: str,
    add_metadata: List[str] = [],
    task_kwargs_dict = {},
    announcement: Optional[str] = None,
    debug: bool = False,
    debug_max: Optional[int] = None,
    workers: int = cpu_count() - 1,
    from_gzip: bool = False,
):
    if announcement:
        print("-"*20)
        print(announcement)
        print("-"*20)

    # Locate pickles
    from_dir = os.path.join(root, get_from)
    pickles = glob(os.path.join(from_dir, '*.pkl.gz'))
    if not from_gzip:
        assert len(pickles) > 0, f"Cannot locate any files in {from_dir}"
        paths = sorted(pickles)[:debug_max]
        assert len(paths) > 1, f"Flagging suspicious behaviour, only 1 or less CIFs present in {from_dir}: {paths}"
    else:
        assert len(pickles) == 1, f"from_gzip flag is raised, but more than one gzip file found in directory"
        with gzip.open(pickles[0], 'rb') as f:
            paths = pickle.load(f)[:debug_max]
            paths = sorted(paths, key=lambda x: x[0])

    # Make output folder
    to_dir = os.path.join(root, save_to)
    os.makedirs(to_dir, exist_ok=True)

    # Open metadata if specified
    if add_metadata:
        metadata_path = os.path.join(root, "metadata.json")
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        for key in add_metadata:
            try:
                task_kwargs_dict[key] = metadata[key]
            except NameError:
                print(f"Could not locate metadata with key: {key} in {metadata_path}")
                pass

    # Check for existing files
    existing_files = set(os.path.basename(f) for f in glob(os.path.join(to_dir, "*.pkl.gz")))

    # Collect tasks
    tasks = []
    pbar = tqdm(desc="Collecting tasks...", total=len(paths), leave=False)
    for path in paths:
        if isinstance(path, str):
            name = os.path.basename(path)
        else:
            name, _ = path
            name = name + '.pkl.gz'
        if name in existing_files:
            pbar.update(1)
            continue
        else:
            tasks.append(
                (path, task_kwargs_dict, debug, to_dir)
            )
            pbar.update(1)
    pbar.close()

    if not tasks:
        print(f"All tasks have already been executed for {save_to}, skipping...", end='\n')
    else:
        # Initialize logger
        log_queue = mp.Queue()
        listener = log_listener(log_queue, to_dir)

        if worker_function == cl_emb_worker:
            # Load the model once
            model_path = task_kwargs_dict['model_path']
            device = task_kwargs_dict.get('device', 'cpu')
            init_worker(log_queue, model_path, device)

            # Process tasks sequentially
            for task in tqdm(tasks, desc="Executing tasks...", leave=False):
                try:
                    worker_function(task)
                except Exception as e:
                    if debug:
                        print(f"Error processing task {task}: {e}")
                    logger = logging.getLogger()
                    logger.exception(f"Exception in worker function for task {task}, error:\n {e}\n\n")
                    continue

            # Stop log listener and flush
            listener.stop()
            logging.shutdown()
        else:
            # Original code for other worker functions
            # Parallel processing of CIF files using multiprocessing
            with Pool(processes=workers, initializer=init_worker, initargs=(log_queue,)) as pool:
                results_iterator = pool.imap_unordered(worker_function, tasks)

                for _ in tqdm(range(len(tasks)), total=len(tasks), desc="Executing tasks...", leave=False):
                    try:
                        results_iterator.next(timeout=60)
                    except TimeoutError as e:
                        continue

            # Stop log listener and flush
            listener.stop()
            logging.shutdown()

    n_files = len(paths)
    n_successful_files = len(glob(os.path.join(to_dir, '*.pkl.gz')))
    print(f"Reduction in dataset: {n_files} samples --> {n_successful_files} samples")

    additional_metadata = {save_to: task_kwargs_dict}
    save_metadata(additional_metadata, root)

    # Free up memory
    del tasks
    gc.collect()

def preprocess_worker(args):
    
    # Extract arguments
    obj, task_dict, debug, to_dir = args

    try:
        # Make structure
        if isinstance(obj, str): # Path
            structure = Structure.from_file(obj)
            cif_name = os.path.basename(obj)
        elif isinstance(obj, tuple): # cif string from pkl
            cif_name, cif_string = obj
            structure = parser_from_string(cif_string).get_structures()[0]
        else:
            raise Exception("Unexpected format found in preprocessing step of {obj}")
        
        # Option for removing structures with occupancies below 1
        if task_dict['remove_occ_less_than_one']:
            for site in structure:
                occ = list(site.species.as_dict().values())[0]
                if occ < 1:
                    raise Exception("Occupancy below 1.0 found")

        # Get stratification key
        strat_key = create_stratification_key(structure, task_dict['spacegroup_strat_group_size'])

        # Get raw content of CIF in string
        cif_string = CifWriter(struct=structure, symprec=0.1).__str__()

        # Extract formula units and remove if Z=0
        formula_units = extract_formula_units(cif_string)
        if formula_units == 0:
            raise Exception()

        # Remove oxidation state information
        cif_string = remove_oxidation_loop(cif_string)

        # Number precision rounding
        cif_string = round_numbers(cif_string, task_dict['decimal_places'])
        cif_string = format_occupancies(cif_string, task_dict['decimal_places'])

        # Add atomic props block
        cif_string = add_atomic_props_block(cif_string)

        # Extract species, spacegroup and composition of crystal structure
        composition = Composition(extract_formula_nonreduced(cif_string)).as_dict()
        species = list(set(composition.keys()))
        spacegroup = extract_space_group_symbol(cif_string)

        # Save output to pickle file
        output_dict = {
            'cif_name': cif_name,
            'cif_string': cif_string,
            'strat_key': strat_key,
            'species': species,
            'spacegroup': spacegroup,
            'composition': composition,
        }
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")

        logger = logging.getLogger()
        logger.exception(f"Exception in worker function pre-processing CIF {cif_name}, with error:\n {e}\n\n")
        
        # Append the file name the failed_files manager list
        #failed_files.append(cif_name)

def descriptor_worker(args):

    # Extract arguments
    from_path, task_dict, debug, to_dir = args
    
    # Open pkl and extract
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']
    cif_string = data['cif_string']
    
    # Get species
    species = task_dict['species']

    try:
        # Load structure and parse to ASE
        ase_structure = parser_from_string(cif_string).get_structures()[0].to_ase_atoms()

        # Setup SOAP object
        soap = SOAP(
            species = species,
            r_cut = task_dict['soap']['r_cut'],
            n_max = task_dict['soap']['n_max'],
            l_max = task_dict['soap']['l_max'],
            sigma = task_dict['soap']['sigma'],
            rbf = task_dict['soap']['rbf'],
            compression = {
                'mode': task_dict['soap']['compression_mode'],
                'weighting': None,
            },
            periodic = task_dict['soap']['periodic'],
            sparse = task_dict['soap']['sparse'],
        )

        # Setup ACSF
        acsf = ACSF(
            species = species,
            r_cut = task_dict['acsf']['r_cut'],
            periodic = task_dict['acsf']['periodic'],
            sparse = task_dict['acsf']['sparse'],
        )

        # Calculate descriptors and save to pkl.gz
        output_dict = {
            'soap': soap.create(ase_structure, centers=[0])[0],
            'acsf': acsf.create(ase_structure, centers=[0])[0],
        }
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function calculating descriptors for CIF {cif_name}, with error:\n {e}\n\n")

def xrd_disc_worker(args):

    # Extract arguments
    from_path, task_dict, debug, to_dir = args

    # Open pkl and extract
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']
    cif_string = data['cif_string']

    try:
        # Load structure and parse to ASE
        structure = parser_from_string(cif_string).get_structures()[0]

        # Init calculator object and calculate XRD pattern
        xrd_calc = XRDCalculator(wavelength=task_dict['wavelength'])
        if task_dict['qmax'] >= ((4 * np.pi) / xrd_calc.wavelength) * np.sin(np.radians(180)):
            two_theta_range = None
        else:
            tth_min = 2 * np.arcsin((task_dict['qmin'] * xrd_calc.wavelength) / (4 * np.pi))
            tth_max = 2 * np.arcsin((task_dict['qmax'] * xrd_calc.wavelength) / (4 * np.pi))
            two_theta_range = (tth_min, tth_max)
        xrd_pattern = xrd_calc.get_pattern(structure, two_theta_range=two_theta_range)

        # Convert units to Q
        theta = np.radians(xrd_pattern.x / 2)
        q_disc = 4 * np.pi * np.sin(theta) / xrd_calc.wavelength # Q = 4 pi sin theta / lambda
        iq_disc = xrd_pattern.y / (np.max(xrd_pattern.y) + 1e-16)

        output_dict = {
            'cif_name': cif_name,
            'xrd_disc': {
                'q': q_disc,
                'iq': iq_disc,
            },
        }

        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function with disc xrd calculation for CIF with name {cif_name}, with error:\n {e}\n\n")
    
def xrd_cont_worker(args):

    # Extract arguments
    from_path, task_dict, debug, to_dir = args

    # Open pkl and extract
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']

    try:
        xrd_dict = disc_to_cont_xrd(
            batch_q = task_dict['q_disc'].unsqueeze(0), 
            batch_iq = task_dict['iq_disc'].unsqueeze(1),
            qmin = task_dict['qmin'],
            qmax = task_dict['qmax'],
            qstep = task_dict['qstep'],
            fwhm_range = task_dict['fwhm_range'],
            eta_range = task_dict['eta_range'],
            noise_range = task_dict['noise_range'],
            intensity_scale_range = None,
            mask_prob = None,
        )

        # Save output to pkl.gz file
        output_dict = {
            'cif_name': cif_name,
            'xrd_cont': xrd_dict,
        }
        
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function with cont xrd calculation for CIF with name {cif_name}, with error:\n {e}\n\n")

def cl_emb_worker(args):

    # Extract arguments
    from_path, task_dict, debug, to_dir = args

    # Open pkl and extract the discrete XRD patterns
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']
    xrd_disc_q = data['xrd_disc']['q']
    xrd_disc_iq = data['xrd_disc']['iq']

    try:
        # Access the global model
        global xrd_encoder
        device = task_dict.get('device', 'cpu')

        # Prepare data
        batch_q = [xrd_disc_q]
        batch_iq = [xrd_disc_iq]

        data_input = (batch_q, batch_iq)

        # Generate embeddings
        with torch.no_grad():
            h, _ = xrd_encoder(data_input, train=False)
            h = h.cpu().numpy()

        # Save output to pkl.gz file
        output_dict = {
            'cif_name': cif_name,
            'xrd_cl_emb': h[0],  # h is of shape (1, embedding_dim)
        }
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function with CL embedding for CIF {cif_name}, error:\n {e}\n\n")

def cif_tokenizer_worker(args):
    
    # Extract arguments
    from_path, task_dict, debug, to_dir = args

    # Open pkl and extract
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']
    cif_string = data['cif_string']

    try:
        # Remove symmetries and header from cif_string before tokenizing
        cif_string = remove_cif_header(cif_string)
        cif_string_reduced = replace_data_formula_with_nonreduced_formula(cif_string)
        cif_string_nosym = replace_symmetry_loop_with_P1(cif_string_reduced)

        # Initialise Tokenizer
        tokenizer = Tokenizer()
        tokenize = tokenizer.tokenize_cif
        encode = tokenizer.encode

        cif_tokenized = encode(tokenize(cif_string_nosym))
    
        # Save output to pickle file
        output_dict = {
            'cif_tokenized': cif_tokenized,
        }
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function with tokenization for CIF with name {cif_name}, with error:\n {e}\n\n")

def xrd_tokenizer_worker(args):
    
    # Extract arguments
    from_path, task_dict, debug, to_dir = args

    # Open pkl and extract
    with gzip.open(from_path, "rb") as f:
        data = pickle.load(f)
    cif_name = data['cif_name']
    xrd_disc = data['xrd_disc']

    try:
        # Convert discrete xrd to string
        xrd_disc_str = "\n".join([f"{x:5.4f}, {y:5.4f}" for (x,y) in zip(xrd_disc['q'], xrd_disc['iq'])])

        # Initialise Tokenizer
        tokenizer = Tokenizer()
        tokenize = tokenizer.tokenize_cif
        encode = tokenizer.encode

        xrd_tokenized = encode(tokenize(xrd_disc_str))
    
        # Save output to pickle file
        output_dict = {
            'xrd_tokenized': xrd_tokenized,
        }
        output_path = os.path.join(to_dir, cif_name + '.pkl.gz')
        safe_pkl_gz(output_dict, output_path)

    except Exception as e:
        if debug:
            print(f"Error processing {cif_name}: {e}")
        logger = logging.getLogger()
        logger.exception(f"Exception in worker function with tokenization for CIF with name {cif_name}, with error:\n {e}\n\n")

def name_and_strat(path):
    with gzip.open(path, 'rb') as f:
        data = pickle.load(f)
        try:
            cif_name = data['cif_name']
            strat_key = data['strat_key']
            return cif_name, strat_key
        except NameError:
            return None

def load_data_from_data_types_list(path_basename, data_types):
    data_dict = {}
    # Loop through data types
    for dct in data_types:
        file_path = os.path.join(dct['dir'], path_basename + '.pkl.gz')
        with gzip.open(file_path, 'rb') as f:
            data = pickle.load(f)
            for key in dct['keys']:
                data_dict[key] = data[key]
    return data_dict

def save_h5(h5_path, cif_names, data_types):

    with h5py.File(h5_path, 'w') as h5f:
        # Placeholder for datasets
        dsets = {} # Stores datasets for each data type
        current_size = 0

        for idx, name in enumerate(tqdm(cif_names, desc=f'Serializing {h5_path}')):
            # Load data for all data types
            try:
                data_dict = load_data_from_data_types_list(name, data_types)
            except:
                print(f"Error in loading: {name}")
                continue

            # Initialise datasets if processing the first file
            if idx == 0:
                for data_key, data_value in data_dict.items():
                    # Determine the data type and create datasets accordinly
                    if isinstance(data_value, np.ndarray):
                        # For numpy arrays
                        data_shape = data_value.shape
                        data_dtype = data_value.dtype
                        if len(data_shape) == 1:
                            # For one-dimensional arrays
                            max_shape = (None, data_shape[0])
                            initial_shape = (0, data_shape[0])
                        else:
                            # For multi-dimensional arrays
                            max_shape = (None,) + data_shape[1:]
                            initial_shape = (0,) + data_shape[1:]

                        dsets[data_key] = h5f.create_dataset(
                            data_key,
                            shape = initial_shape,
                            maxshape = max_shape,
                            chunks = True,
                            dtype = data_dtype,
                        )
                    elif isinstance(data_value, str):
                        # For strings
                        dt = h5py.string_dtype(encoding='utf-8')
                        dsets[data_key] = h5f.create_dataset(
                            data_key,
                            shape = (0,),
                            maxshape = (None,),
                            dtype = dt,
                        )
                    elif isinstance(data_value, int):
                        # For integers
                        dsets[data_key] = h5f.create_dataset(
                            data_key,
                            shape = (0,),
                            maxshape = (None,),
                            dtype = 'int32',
                        )
                    elif isinstance(data_value, float):
                        # For floats
                        dsets[data_key] = h5f.create_dataset(
                            data_key,
                            shape = (0,),
                            maxshape = (None,),
                            dtype = 'float32',
                        )
                    elif isinstance(data_value, (list, set)):
                        # Determine if the list contains numbers or strings
                        if all(isinstance(item, (int, float, np.number)) for item in data_value):
                            # For lits of numbers
                            if all(isinstance(item, int) for item in data_value):
                                dt = h5py.vlen_dtype(np.dtype('int32'))
                            else:
                                dt = h5py.vlen_dtype(np.dtype('float32'))
                            
                            dsets[data_key] = h5f.create_dataset(
                                data_key,
                                shape = (0,),
                                maxshape = (None,),
                                dtype = dt,
                            )
                        elif all(isinstance(item, str) for item in data_value):
                            # For lists of strings
                            dt = h5py.string_dtype(encoding='utf-8')
                            dsets[data_key] = h5f.create_dataset(
                                data_key,
                                shape = (0,),
                                maxshape = (None,),
                                dtype = dt,
                            )
                        else:
                            raise TypeError(f"Unsupported data type for key '{data_key}': {type(data_value)}")
                    elif isinstance(data_value, dict):
                        # For dicts of arrays
                        # A dataset for each of the arrays in the dict
                        dt = h5py.vlen_dtype(np.dtype('float32'))
                        for key, values in data_value.items():
                            dsets[data_key + '.' + key] = h5f.create_dataset(
                                data_key + '.' + key,
                                shape = (0,),
                                maxshape = (None,),
                                dtype = dt,
                            )
                    else:
                        raise TypeError(f"Unsupported data type for key '{data_key}': {type(data_value)}")

            # Append data to datasets
            for data_key, data_value in data_dict.items():
                if isinstance(data_value, dict):
                    for key, values in data_value.items():
                        dset = dsets[data_key + '.' + key]
                        dset.resize(current_size + 1, axis=0)
                        dset[current_size] = values
                else:
                    dset = dsets[data_key]
                    # Resize dataset to accomodate new data
                    dset.resize(current_size + 1, axis=0)
                    # Assign data based on type
                    if isinstance(data_value, np.ndarray):
                        dset[current_size] = data_value
                    elif isinstance(data_value, (str, int, float)):
                        dset[current_size] = data_value
                    elif isinstance(data_value, (list, set)):
                        if all(isinstance(item, (int, float, np.number)) for item in data_value):
                            # Convert list to numpy array
                            dset[current_size] = np.array(data_value)
                        elif all(isinstance(item, str) for item in data_value):
                            # Serialize the list to a JSON string
                            dset[current_size] = json.dumps(list(data_value))
                        else:
                            raise TypeError(f"Unsupported data type for key '{data_key}': {type(data_value)}")
                    else:
                        raise TypeError(f"Unsupported data type for key '{data_key}': {type(data_value)}")
            current_size += 1

def serialize(root, workers, seed):

    # Locate available data TODO make this automatic based on folder names etc.
    pre_dir = os.path.join(root, "preprocessed")
    pre_paths = glob(os.path.join(pre_dir, "*.pkl.gz"))
    assert len(pre_paths) > 0, f"No preprocessing files found in {pre_dir}"
    dataset_size = len(pre_paths)
    
    # Make output folder
    ser_dir = os.path.join(root, "serialized")
    os.makedirs(ser_dir, exist_ok=True)
    
    # Retrieve all cif names and stratification keys
    with Pool(processes=workers) as pool:
        results = list(tqdm(pool.imap(name_and_strat, pre_paths), total=len(pre_paths), desc="Retrieving names and stratification keys", leave=False))

    # Seperate cif neams and stratification keys
    cif_names = [item[0] for item in results]
    strat_keys = [item[1] for item in results]
    
    # Create data splits
    train_size = int(0.9 * dataset_size)
    val_size = int(0.075 * dataset_size)
    test_size = dataset_size - train_size - val_size

    print("Train size:", train_size)
    print("Val size:", val_size)
    print("Test size:", test_size)

    cif_names_temp, cif_names_test, strat_keys_temp, _ = train_test_split(
        cif_names, strat_keys, test_size = test_size, stratify = strat_keys, random_state = seed,
    )
    cif_names_train, cif_names_val = train_test_split(
        cif_names_temp, test_size = test_size, stratify = strat_keys_temp, random_state = seed,
    )

    # Data types
    data_types = []

    # Preprocessed
    data_types.append({'dir': pre_dir, 'keys': ['cif_name', 'cif_string', 'strat_key', 'species', 'spacegroup']})
    
    desc_dir = os.path.join(root, "descriptors")
    desc_paths = glob(os.path.join(desc_dir, "*.pkl.gz"))
    if len(desc_paths) > 0:
        data_types.append({'dir': desc_dir, 'keys': ['soap', 'acsf']})

    xrd_disc_dir = os.path.join(root, "xrd_disc")
    xrd_disc_paths = glob(os.path.join(xrd_disc_dir, "*.pkl.gz"))
    if len(xrd_disc_paths) > 0:
        data_types.append({'dir': xrd_disc_dir, 'keys': ['xrd_disc']})

    xrd_cont_dir = os.path.join(root, "xrd_cont")
    xrd_cont_paths = glob(os.path.join(xrd_cont_dir, "*.pkl.gz"))
    if len(xrd_cont_paths) > 0:
        data_types.append({'dir': xrd_cont_dir, 'keys': ['xrd_cont']})
    
    cl_dir = os.path.join(root, "cl_embeddings")
    cl_paths = glob(os.path.join(cl_dir, "*.pkl.gz"))
    if len(cl_paths) > 0:
        data_types.append({'dir': cl_dir, 'keys': ['xrd_cl_emb']})

    cif_token_dir = os.path.join(root, "cif_tokenized")
    cif_token_paths = glob(os.path.join(cif_token_dir, "*.pkl.gz"))
    if len(cif_token_paths) > 0:
        data_types.append({'dir': cif_token_dir, 'keys': ['cif_tokenized']})

    xrd_token_dir = os.path.join(root, "xrd_tokenized")
    xrd_token_paths = glob(os.path.join(xrd_token_dir, "*.pkl.gz"))
    if len(xrd_token_paths) > 0:
        data_types.append({'dir': xrd_token_dir, 'keys': ['xrd_tokenized']})

    for cif_names, split_name in zip([cif_names_train, cif_names_val, cif_names_test], ['train', 'val', 'test']):
        h5_path = os.path.join(ser_dir, f'{split_name}.h5')
        save_h5(h5_path, cif_names, data_types)

def retrieve_worker(args):
    
    # Extract args
    path, key = args

    # Open pkl and extract
    with gzip.open(path, "rb") as f:
        data = pickle.load(f)[key]

    return data

def collect_data(root, get_from, key, workers=cpu_count() - 1):

    # Find paths
    paths = glob(os.path.join(root, get_from, "*.pkl.gz"))
    args = [(path, key) for path in paths] 

    # Define output
    output = []

    # Parallel process retrieving the results
    ctx = mp.get_context('spawn')
    with ctx.Pool(processes=workers) as pool:
        key_data = list(tqdm(pool.imap(retrieve_worker, args), total=len(paths), desc=f"Retrieving {key} from {get_from}...", leave=False))
        
    return key_data

if __name__ == "__main__":

    import multiprocessing as mp
    mp.set_start_method('spawn', force=True)

    parser = argparse.ArgumentParser(description="Prepare custom CIF files and save to a tar.gz file.")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--data-dir", type=str, help="Path to the outer data directory", required=True)
    parser.add_argument("--name", "-n", type=str, help="Name of data preparation", required=True)
    parser.add_argument("--group-size", type=int, help="Spacegroup group size for stratification", default=10)
    parser.add_argument("--decimal-places", type=int, help="Number of decimal places for floats in CIF files", default=4)
    parser.add_argument("--include-occ", help="Include structures with occupancies less than one", action="store_false")

    parser.add_argument("--preprocess", help="preprocess files", action="store_true")
    parser.add_argument("--desc", help="calculate descriptors", action="store_true")  # Placeholder for future implementation
    parser.add_argument("--xrd-disc", help="calculate discrete XRD", action="store_true")  # Placeholder for future implementation
    parser.add_argument("--xrd-cont", help="calculate continuous XRD", action="store_true")  # Placeholder for future implementation
    parser.add_argument("--tokenize-cif", help="tokenize CIFs", action="store_true")  # Placeholder for future implementation
    parser.add_argument("--tokenize-xrd", help="tokenize XRDs", action="store_true")  # Placeholder for future implementation
    parser.add_argument("--serialize", help="serialize data by hdf5 convertion", action="store_true")  # Placeholder for future implementation

    parser.add_argument("--debug-max", help="Debug-feature: max number of files to process", type=int, default=0)
    parser.add_argument("--debug", help="Debug-feature: whether to print debug messages", action="store_true")
    parser.add_argument("--workers", help="Number of workers for each processing step", type=int, default=0)
    parser.add_argument("--raw-from-gzip", help="Whether raw CIFs come packages in gzip pkl", action="store_true")
    parser.add_argument("--save-species-to-metadata", help="Extraordinary save of species to metadata", action="store_true")

    parser.add_argument("--cl-emb", help="Generate contrastive learning embeddings", action="store_true")
    parser.add_argument("--model-path", type=str, help="Path to the trained model checkpoint for generating embeddings")

    args = parser.parse_args()

    # Remove occ
    args.remove_occ = not args.include_occ

    # workers
    if args.workers == 0:
        args.workers = cpu_count() - 1
    else:
        args.workers = min(cpu_count() - 1, args.workers)

    # Make data prep directory and update data_dir
    args.data_dir = os.path.join(args.data_dir, args.name)
    os.makedirs(args.data_dir, exist_ok=True)

    # Adjust debug_max if no limit is specified
    if args.debug_max == 0:
        args.debug_max = None

    if args.preprocess:
        preprocess_dict = {
            'spacegroup_strat_group_size': args.group_size,
            'decimal_places': args.decimal_places,
            'remove_occ_less_than_one': args.remove_occ,
        }
        run_subtasks(
            root = args.data_dir, 
            worker_function = preprocess_worker,
            get_from = "../",
            save_to = "preprocessed",
            task_kwargs_dict = preprocess_dict,
            announcement = "PREPROCESSING",
            debug = True,
            workers = args.workers,
            from_gzip = args.raw_from_gzip,
            debug_max = args.debug_max,
        )
        
        # Save species to metadata
        species = collect_data(
            root = args.data_dir,
            get_from = "preprocessed",
            key = "species",
        ) # Returns a list of lists
        species = {'species': list(set([item for sublist in species for item in sublist]))}
        save_metadata(species, args.data_dir)

    if args.save_species_to_metadata:
        # Save species to metadata
        species = collect_data(
            root = args.data_dir,
            get_from = "preprocessed",
            key = "species",
        ) # Returns a list of lists
        species = {'species': list(set([item for sublist in species for item in sublist]))}
        save_metadata(species, args.data_dir)
    
    if args.desc:
        descriptor_dict = {
            'soap': {
                'r_cut': 6.0,
                'n_max': 2,
                'l_max': 5,
                'sigma': 1.0,
                'rbf': 'gto',
                'compression_mode': 'crossover',
                'periodic': True,
                'sparse': False,
            },
            'acsf': {
                'r_cut': 6.0,
                'periodic': True,
                'sparse': False,
            },
        }
        run_subtasks(
            root = args.data_dir, 
            worker_function = descriptor_worker,
            get_from = "preprocessed",
            save_to = "descriptors",
            add_metadata = ["species"],
            task_kwargs_dict = descriptor_dict,
            announcement = "DESCRIPTOR CALCULATIONS",
            debug = True,
            workers = args.workers,
        )

    if args.xrd_disc:
        xrd_dict = {
            'wavelength': 'CuKa',
            'qmin': 0.0,
            'qmax': 10.0,
            'qstep': 0.01,
        }
        run_subtasks(
            root = args.data_dir, 
            worker_function = xrd_disc_worker,
            get_from = "preprocessed",
            save_to = "xrd_disc",
            task_kwargs_dict = xrd_dict,
            announcement = "XRD DISC CALCULATIONS",
            debug = True,
            workers = args.workers,
        )
    
    if args.xrd_cont:
        xrd_dict = {
            'wavelength': 'CuKa',
            'qmin': 0.0,
            'qmax': 10.0,
            'qstep': 0.01,
            'fwhm_range': (0.05, 0.05),
            'eta_range': (0.5, 0.5),
            'noise_range': (0.0, 0.0),
        }
        run_subtasks(
            root = args.data_dir, 
            worker_function = xrd_cont_worker,
            get_from = "xrd_disc",
            save_to = "xrd_cont",
            task_kwargs_dict = xrd_dict,
            announcement = "XRD CONT CALCULATIONS",
            debug = True,
            workers = args.workers,
        )

    if args.cl_emb:
        if not args.model_path:
            parser.error("--model_path is required when --cl_emb is specified.")
        cl_emb_dict = {
            'model_path': args.model_path,  # Path to your trained model checkpoint
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        }
        run_subtasks(
            root = args.data_dir,
            worker_function = cl_emb_worker,
            get_from = "xrd",
            save_to = "cl_embeddings",
            task_kwargs_dict = cl_emb_dict,
            announcement = "GENERATING CL EMBEDDINGS",
            debug = True,
            workers = args.workers,
        )

    if args.tokenize_cif:
        run_subtasks(
            root = args.data_dir, 
            worker_function = cif_tokenizer_worker,
            get_from = "preprocessed",
            save_to = "cif_tokenized",
            announcement = "TOKENIZING CIFs",
            debug = True,
            workers = args.workers,
        )

    if args.tokenize_xrd:
        run_subtasks(
            root = args.data_dir, 
            worker_function = xrd_tokenizer_worker,
            get_from = "xrd",
            save_to = "xrd_tokenized",
            announcement = "TOKENIZING XRDs",
            debug = True,
            workers = args.workers,
        )
    
    if args.serialize:
        serialize(args.data_dir, args.workers, args.seed)

    # Store all arguments passed to the main function in centralized metadata
    metadata = {
        "arguments": vars(args)
    }

    # Finalize metadata saving after all processing steps
    save_metadata(metadata, args.data_dir)
